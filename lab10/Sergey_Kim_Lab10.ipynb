{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                             classification_report)\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0                           Wow... Loved this place.  1\n",
       "1                                 Crust is not good.  0\n",
       "2          Not tasty and the texture was just nasty.  0\n",
       "3  Stopped by during the late May bank holiday of...  1\n",
       "4  The selection on the menu was great and so wer...  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('yelp_labelled.txt', header=None, sep='\\t')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = data[0]\n",
    "y = data[1]\n",
    "X_train, X_test, y_train, y_test = skm.train_test_split(X_text, y, test_size=0.2, stratify=y, random_state=0)\n",
    "X_text_train, X_validation, y_text_train, y_validation = skm.train_test_split(X_train, y_train, test_size=0.125, stratify=y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fasttext = [model.get_sentence_vector(text) for text in X_text_train]\n",
    "X_validation_fasttext = [model.get_sentence_vector(text) for text in X_validation]\n",
    "X_test_fasttext = [model.get_sentence_vector(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DhM\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "X_train_sentence = sentence_model.encode(X_text_train.tolist(), convert_to_tensor=True)\n",
    "X_validation_sentence = sentence_model.encode(X_validation.tolist(), convert_to_tensor=True)\n",
    "X_test_sentence = sentence_model.encode(X_test.tolist(), convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Validation Accuracy with FastText + Logistic Regression: 0.850\n",
      "Test Accuracy with FastText + Logistic Regression: 0.755\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define hyperparameters to tune using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Different solvers\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters on the validation set\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_fasttext, y_text_train)\n",
    "\n",
    "# Get the best model and evaluate on the validation data\n",
    "best_lr = grid_search.best_estimator_\n",
    "y_validation_pred = best_lr.predict(X_validation_fasttext)\n",
    "print(\"Validation Accuracy with FastText + Logistic Regression: %.3f\" % accuracy_score(y_validation, y_validation_pred))\n",
    "\n",
    "# Apply the best model to the test data\n",
    "y_test_pred = best_lr.predict(X_test_fasttext)\n",
    "print(\"Test Accuracy with FastText + Logistic Regression: %.3f\" % accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you can see, there is some overfitting for logistic regression, so test set accuracy is not high. However, this is a model that already has regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Validation Accuracy with Sentence Transformers + Logistic Regression: 0.870\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],        # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Different solvers for logistic regression\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV on the validation set\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_sentence, y_text_train)\n",
    "\n",
    "# Get the best model and evaluate on the validation data\n",
    "best_lr = grid_search.best_estimator_\n",
    "y_validation_pred = best_lr.predict(X_validation_sentence)\n",
    "print(\"Validation Accuracy with Sentence Transformers + Logistic Regression: %.3f\" % accuracy_score(y_validation, y_validation_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Sentence Transformers + Logistic Regression: 0.880\n"
     ]
    }
   ],
   "source": [
    "# Apply the best model to the test data\n",
    "y_test_pred = best_lr.predict(X_test_sentence)\n",
    "print(\"Test Accuracy with Sentence Transformers + Logistic Regression: %.3f\" % accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence transformed data shows better results for test than validation, but that can be explained because validation set is too small and test set is a little bigger (10 validation to 20 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Validation Accuracy: 0.78\n",
      "Gradient Boosting Test Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       100\n",
      "           1       0.80      0.78      0.79       100\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.79      0.79       200\n",
      "weighted avg       0.79      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb_params = {'n_estimators': [50, 100, 150],\n",
    "             'learning_rate': [0.01, 0.1, 0.2],\n",
    "             'max_depth': [3, 5, 7]}\n",
    "gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='accuracy')\n",
    "gb_grid.fit(X_train_fasttext, y_text_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "best_gb = gb_grid.best_estimator_\n",
    "gb_validation_pred = best_gb.predict(X_validation_fasttext)\n",
    "print(\"Gradient Boosting Validation Accuracy:\", accuracy_score(y_validation, gb_validation_pred))\n",
    "\n",
    "# Apply on test data\n",
    "gb_test_pred = best_gb.predict(X_test_fasttext)\n",
    "print(\"Gradient Boosting Test Accuracy:\", accuracy_score(y_test, gb_test_pred))\n",
    "print(classification_report(y_test, gb_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I chose gradient boosting, because it can capture non linear relationships well. Sentiment analysis involves capturing complex relationships in the data, hence gradient boosting is useful. Plus fasttext and sentence transformer encode data making it complex, which makes gradient boosting useful. \n",
    "# As expected, fasttext model is not working very well given the accuracy of 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Validation Accuracy: 0.84\n",
      "Gradient Boosting Test Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       100\n",
      "           1       0.88      0.90      0.89       100\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.89      0.89      0.89       200\n",
      "weighted avg       0.89      0.89      0.89       200\n",
      "\n",
      "Best Parameters for Logistic Regression with FastText embeddings: {'C': 10, 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb_params = {'n_estimators': [50, 100, 150],\n",
    "             'learning_rate': [0.01, 0.1, 0.2],\n",
    "             'max_depth': [3, 5, 7]}\n",
    "gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='accuracy')\n",
    "gb_grid.fit(X_train_sentence, y_text_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "best_gb = gb_grid.best_estimator_\n",
    "gb_validation_pred = best_gb.predict(X_validation_sentence)\n",
    "print(\"Gradient Boosting Validation Accuracy:\", accuracy_score(y_validation, gb_validation_pred))\n",
    "\n",
    "# Apply on test data\n",
    "gb_test_pred = best_gb.predict(X_test_sentence)\n",
    "print(\"Gradient Boosting Test Accuracy:\", accuracy_score(y_test, gb_test_pred))\n",
    "print(classification_report(y_test, gb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# However, given the sentence transformed data, the test accuracy is highest among all models so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 17:29:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 2.92MB/s]                    \n",
      "2024-11-14 17:29:08 INFO: Downloaded file to C:\\Users\\DhM\\stanza_resources\\resources.json\n",
      "2024-11-14 17:29:08 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-14 17:29:08 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | combined       |\n",
      "| mwt       | combined       |\n",
      "| sentiment | sstplus_charlm |\n",
      "==============================\n",
      "\n",
      "2024-11-14 17:29:08 INFO: Using device: cpu\n",
      "2024-11-14 17:29:08 INFO: Loading: tokenize\n",
      "c:\\Users\\DhM\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-14 17:29:08 INFO: Loading: mwt\n",
      "c:\\Users\\DhM\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-14 17:29:08 INFO: Loading: sentiment\n",
      "c:\\Users\\DhM\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\stanza\\models\\classifiers\\trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\DhM\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\DhM\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "2024-11-14 17:29:09 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=True)\n",
    "\n",
    "collection = '\\n\\n'.join(X_test)\n",
    "doc = nlp(collection)\n",
    "\n",
    "stanza_predictions = []\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    if sentence.sentiment == 0 or sentence.sentiment == 1:\n",
    "        stanza_predictions.append(0)\n",
    "    if sentence.sentiment == 2:\n",
    "        stanza_predictions.append(1)\n",
    "\n",
    "print('Test set accuracy: %.3f' % accuracy_score(y_test, stanza_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Despite gradient boosting having higher scores than logistic regressor, stanza model which is best suited for sentiment analysis outperforms gradient boosting. Perhaps the data is not complex enough for gradeint boosting to work better. However, there is a possibility that if the hyperparameter range was taken broader, gradient boosting score could have increased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
